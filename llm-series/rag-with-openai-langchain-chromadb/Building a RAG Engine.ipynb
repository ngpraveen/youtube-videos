{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcbf8f0-1764-4533-8ea1-b0ededa4b085",
   "metadata": {},
   "source": [
    "# Build a Simple RAG Engine Using LangChain, ChromaDB and OpenAI\n",
    "\n",
    "This notebook is a demo for building a simple RAG engine. \n",
    "OpenAI's embedding is used to vectorize documents. \n",
    "The vectors are stored locally in a Chroma database. \n",
    "Integration is done using Python and LangChain. \n",
    "\n",
    "You will need \n",
    "\n",
    "LangChain is a heavily developed framework, and they do make many changes, \n",
    "especially with versions < 1.0.0. \n",
    "**Make sure that the exact versions of packages listed in requirements.txt are installed.**\n",
    "This code runs using their first stable version 1.0.0, \n",
    "so hopefully newer versions the authors publish will be consistent with 1.0.0. \n",
    "But it is always a good idea to use the exact same versions of all packages. \n",
    "\n",
    "Using a virtual environment is highly reccommeded. I use [uv](https://astral.sh/blog/uv). \n",
    "If you haven't used it before, I have a [quickstart demo post](https://praveenng.medium.com/uv-a-fast-alternative-to-pip-6f1d8c4a30aa) on Medium.com.\n",
    "\n",
    "I have another Medium.com post on building RAG Engine, which is very consistent with this notebook. If you like, you can read it [here](https://medium.com/@praveenng/creating-a-vector-database-for-rag-471aca771bce). \n",
    "However, note that the versions of packages I used in that article are slightly different, and hence the code has slight changes. \n",
    "\n",
    "Finally, the goal of this notebook is to get started with a simple RAG engine. In a production environment, things are much more complicated, as you know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd8c2e6-acae-4b1c-bd71-9aa019835ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pysqlite3 instead of sqlite3 (for ChromaDB)\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43d344a-bd53-427d-890f-9669dfe14139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f205577a-427a-4b70-a980-75333e87c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir where documents are stored\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# file extension ('*.txt', '*.md' etc.)\n",
    "# all files with these extensions will be read and embedded.\n",
    "FILE_EXT = ['*.md']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af27a7a-8e05-49ff-ad72-7c6ffa6b0106",
   "metadata": {},
   "source": [
    "### 1. Builiding Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2874090-eee7-42a0-8c0e-c06d714c9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 docuemnt(s) is(are) loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the document from folder 'data'\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(DATA_DIR, glob=FILE_EXT)\n",
    "docs = loader.load()\n",
    "print(f\"{len(docs)} docuemnt(s) is(are) loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfb9e93-d7c2-431b-9876-ff704e0bd9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docuemnts are split into 1719 chunks.\n"
     ]
    }
   ],
   "source": [
    "# split loaded document into smaller chunks. \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Docuemnts are split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531ef201-46b1-4870-a941-a92351699684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed (vectorize) chunks using OpenAIEmbeddings and store in chromadb\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4aee88-6022-4811-838d-5452eecbfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embedding_function,\n",
    "    persist_directory='chroma',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eefabeb-ea6b-45ca-8032-164fb392e328",
   "metadata": {},
   "source": [
    "### 2. Testing Retrieval\n",
    "We run a query and see if we can retrieve relevant documents from the vector database based on similarity search. \n",
    "Below we get the 3 best documents that has the highest similarity with the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bffb535f-455d-4cb2-898c-a91df09a0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query \n",
    "COLLECTION_NAME = 'langchain'\n",
    "K_DOCUMENTS = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71087b27-b558-4106-abf6-0dbe9f571d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "            persist_directory='chroma',\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "            query=my_query, \n",
    "            k=K_DOCUMENTS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06b8bb8-7ca1-4186-bd6c-434ef8a7e2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "  Content: year, Raskolnikov had got the old man into a hospital and paid for his funeral when he died. Raskolnikov’s landlady bore witness, too, that when they had lived in another house at Five Corners, Raskolnikov had rescued two little children from a house on fire and was burnt in doing so. This was investigated and fairly well confirmed by many witnesses. These facts made an impression in his favour....\n",
      "  Source/Metadata: {'source': 'data/crime_and_punishment.md', 'start_index': 1104781}\n",
      "---\n",
      "\n",
      "\n",
      "Result 2:\n",
      "  Content: Five months after Raskolnikov’s confession, he was sentenced. Razumihin and Sonia saw him in prison as often as it was possible. At last the moment of separation came. Dounia swore to her brother that the separation should not be for ever, Razumihin did the same. Razumihin, in his youthful ardour, had firmly resolved to lay the foundations at least of a secure livelihood during the next three or four years, and saving up a certain sum, to emigrate to Siberia, a country rich in every natural resource and in need of workers, active men and capital. There they would settle in the town where Rodya was and all together would begin a new life. They all wept at parting....\n",
      "  Source/Metadata: {'start_index': 1108872, 'source': 'data/crime_and_punishment.md'}\n",
      "---\n",
      "\n",
      "\n",
      "Result 3:\n",
      "  Content: Meanwhile Raskolnikov had squeezed in and stooped closer over him. The lantern suddenly lighted up the unfortunate man’s face. He recognised him.\n",
      "\n",
      "“I know him! I know him!” he shouted, pushing to the front. “It’s a government clerk retired from the service, Marmeladov. He lives close by in Kozel’s house.... Make haste for a doctor! I will pay, see?” He pulled money out of his pocket and showed it to the policeman. He was in violent agitation.\n",
      "\n",
      "The police were glad that they had found out who the man was. Raskolnikov gave his own name and address, and, as earnestly as if it had been his father, he besought the police to carry the unconscious Marmeladov to his lodging at once....\n",
      "  Source/Metadata: {'source': 'data/crime_and_punishment.md', 'start_index': 376813}\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Content: {doc.page_content}...\")\n",
    "    print(f\"  Source/Metadata: {doc.metadata}\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e117c7d-5af5-4595-9482-f51070044c56",
   "metadata": {},
   "source": [
    "You can see that Result 1 (results\\[0]) contains info related to the query. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7a5eba-b2aa-43c5-a18a-717cf0ca67f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raskolnikov had rescued two little children from a house on fire and was burnt in doing so. This was investigated and fairly well confirmed by many witnesses. These facts made an impression in his favour.\n"
     ]
    }
   ],
   "source": [
    "print(results[0].page_content[194:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c699955-cbe1-41cb-b037-fbe07adf1d26",
   "metadata": {},
   "source": [
    "<br> But wouldn't it be nice if we get the answer direclty without having to go through the documents? That's what a RAG engine does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8493800-02da-4258-ba94-5193f9dd0afd",
   "metadata": {},
   "source": [
    "### 3. Building RAG Engine\n",
    "\n",
    "We now put everything together. The steps include building a RAG chain that accepts a query, embed (vectorize) the query, do a similarty search of the embedded query against documents in the database and retrieve relevant documents. Finally, we pass the query and the context (relevant documents) to an LLM. The LLM, based on the query and the context, generates an answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ee5fb2-91b0-4e6c-b5cf-d7ff57b0d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e704b1c7-b308-4b97-a5ba-48f5ec87f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "583707a7-aee9-4c6e-af98-e1b8296cb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"As an assistant for answering questions related to the documents, \n",
    "use the following pieces of retrieved context to answer the question.\n",
    "If the question cannot be answered based on the context, just say that you don't have the information to answer the question.\n",
    "Use ten sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1518bafb-4a70-4da9-9f8b-e200814c6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9b3719e-3f22-4e43-a795-64438ce9a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55377009-4276-4672-9c8e-e5d7a9bf6d95",
   "metadata": {},
   "source": [
    "### 4. Testing RAG Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8eff9c-d814-4c6d-91b1-5b4e342738c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raskolnikov rescued two little children from a house on fire when he lived at Five Corners.\n"
     ]
    }
   ],
   "source": [
    "# your query about the document\n",
    "my_query = 'Whom did Raskolnikov rescue when he lived at Five Corners?'\n",
    "\n",
    "final_response = rag_chain.invoke(my_query)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49651380-601c-4fd0-af2a-35e565604226",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "That's great. But, what if we ask a question that is not related to the documents in the database? We prompted the LLM to say that it didn't know. Let's check it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81982100-ac8c-4aeb-a65e-fa717161ece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have the information to answer the question regarding the 6th president of the US based on the provided context.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your query about the document\n",
    "my_query = 'Who is the 6th president of the US?'\n",
    "\n",
    "rag_chain.invoke(my_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97d48d-5d19-435a-8798-2026ef2e0b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b3000-0b7f-4721-a955-4c6112f348c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62610091-fbdc-467d-818a-7a914c170adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
