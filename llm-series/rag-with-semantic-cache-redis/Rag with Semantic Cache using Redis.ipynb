{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Leveraging LLMs with Semantic Caching using Redis\n",
        "\n",
        "This notebook demonstrates how to utilize caching to minimize repeated calls to Large Language Model (LLM) APIs for identical queries. We will implement caching using a Redis Cloud database.\n",
        "\n",
        "In this demonstration, we will explore two types of caching: **standard caching** and **semantic caching**.\n",
        "\n",
        "### Standard Caching\n",
        "In standard caching, if a query is repeated, the system will serve the response from the cache, avoiding an additional call to the LLM API. However, if two queries are phrased differently but have the same meaning—for example, \"What is the capital of France?\" and \"Which city is the capital of France?\"—they will be treated as distinct queries. This means the LLM will be queried for each variation.\n",
        "\n",
        "### Semantic Caching\n",
        "Semantic caching, on the other hand, is more sophisticated. It recognizes that queries with similar meanings, even if expressed in different wording, refer to the same request. In the example above, both queries would be understood as asking for the capital of France, and the response would be fetched from the cache rather than querying the LLM again.\n",
        "\n",
        "### Prerequisites\n",
        "To follow along, you will need to create a [free Redis account](https://redis.io/try-free/). Additionally, an OpenAI API key is required for accessing their embeddings model. Note that OpenAI's API is not free, but you can create an account with as little as $5. This project should incur only minimal costs, leaving the remainder of your credit for future use.\n"
      ],
      "metadata": {
        "id": "Sz5w30FLva9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lj8ATat2UfP0"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install -q langchain-core==1.1.1 langchain-openai==1.1.1 langchain-redis==0.2.5 redis==6.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import os\n",
        "import time\n",
        "import redis\n",
        "\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
        "from langchain_redis import RedisCache, RedisSemanticCache\n"
      ],
      "metadata": {
        "id": "nFak5I0VUxxk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OpenAI API Key and Redis password.\n",
        "# Works only on Google Colab.\n",
        "# Make sure you add REDIS_PASSWORD and\n",
        "# OPENAI_API_KEY to Google Colab's Secrets.\n",
        "# If you use a different notebook, modify this cell.\n",
        "# Use your own Redis Url.\n",
        "\n",
        "# if you use Google Colab\n",
        "from google.colab import userdata\n",
        "REDIS_URL = \"redis-19585.c90.us-east-1-3.ec2.cloud.redislabs.com\"\n",
        "REDIS_PASSWORD = userdata.get(\"REDIS_PASSWORD\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# if you don't use Google Colab\n",
        "# REDIS_URL = \"\"\n",
        "# REDIS_PASSWORD = \"\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "7NKzs7EeVPkr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Caching"
      ],
      "metadata": {
        "id": "CtfDu17-yRTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "redis_client = redis.Redis(\n",
        "    host=REDIS_URL,\n",
        "    port=19585,\n",
        "    decode_responses=True,\n",
        "    username=\"default\",\n",
        "    password=REDIS_PASSWORD,\n",
        ")\n",
        "\n",
        "response = redis_client.ping()\n",
        "print(response) # Prints True if everything is configured correctly."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBxkTapRWUoq",
        "outputId": "4e5c9708-6eea-44b8-8230-d8f8c8713766"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set standard cache for the LLM\n",
        "redis_cache = RedisCache(redis_client=redis_client)\n",
        "set_llm_cache(redis_cache)\n",
        "llm = OpenAI(temperature=0.0)"
      ],
      "metadata": {
        "id": "1q_1ArqHYY8L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom decorator for finding the execution time\n",
        "def timeit(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Running function {func.__name__}.\")\n",
        "        print(f\"It took {end_time - start_time:.2f} seconds to run.\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# utility function to run query & calculate execution time\n",
        "@timeit\n",
        "def run_query(query):\n",
        "    result = llm.invoke(query)\n",
        "    return result"
      ],
      "metadata": {
        "id": "gTecImisfUy7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Which city is the capital of France?\"\n",
        "print(run_query(query1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVGX4DZQgeGv",
        "outputId": "13964495-14aa-4db6-d870-4911370adaa1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running function run_query.\n",
            "It took 0.53 seconds to run.\n",
            "\n",
            "\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat with same query\n",
        "query2 = \"Which city is the capital of France?\"\n",
        "print(run_query(query2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB3mtR0Vxf9T",
        "outputId": "7bd302c5-9d07-4baf-db40-47a525721dc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running function run_query.\n",
            "It took 0.03 seconds to run.\n",
            "\n",
            "\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat with semantically similar query\n",
        "query3 = \"Which is the capital of France?\"\n",
        "print(run_query(query3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17ikOJ1Rgj5u",
        "outputId": "7b01bf1a-b322-4d8d-d261-044871c188c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running function run_query.\n",
            "It took 0.84 seconds to run.\n",
            "\n",
            "\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When query1 is served, the response is stored to stanadrd cache. Since query1 and query2 are identical, cache is used to serve a response to query2, and the response is served faster.\n",
        "\n",
        "query3 is phrased differently, and standard cache considers it to be different from query1 and query2. So, a new LLM call is invoked and it takes longer to serve the response.\n",
        "\n",
        "Next, we will see how with semantic caching, two differently phrased queries with same meaning are treated as the same query."
      ],
      "metadata": {
        "id": "mTuDk8G85CWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Caching"
      ],
      "metadata": {
        "id": "zvHY7yWK0fBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "semantic_cache = RedisSemanticCache(\n",
        "    redis_client=redis_client,\n",
        "    embeddings=embeddings,\n",
        "    # distance_threshold=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "set_llm_cache(semantic_cache)"
      ],
      "metadata": {
        "id": "9R_nl6l4h7FH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a new query\n",
        "query4 = \"Who is the only woman to hold the office of the Chancellor of Germany\"\n",
        "print(run_query(query4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjIxutuvjJgp",
        "outputId": "dab7e91b-ae94-451b-b7e3-efbea37f34e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running function run_query.\n",
            "It took 1.27 seconds to run.\n",
            "\n",
            "\n",
            "Angela Merkel is the only woman to hold the office of the Chancellor of Germany. She has been in office since 2005 and is currently serving her fourth term.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat with a semantically similar query\n",
        "query5 = \"What is the name of Germany's only female Chancellor?\"\n",
        "print(run_query(query5))"
      ],
      "metadata": {
        "id": "EhJ3_9aSjaCr",
        "outputId": "1be2c48f-8bbd-4d05-d909-d0495ae77fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running function run_query.\n",
            "It took 0.23 seconds to run.\n",
            "\n",
            "\n",
            "Angela Merkel is the only woman to hold the office of the Chancellor of Germany. She has been in office since 2005 and is currently serving her fourth term.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With semantic caching, the semantic meaning of queries are compared, and hence query4 and query5 are considered as \"same\". The semantic meanings are compared using similarity search, so the response takes longer than the standard cache.\n",
        "\n",
        "Note that in this demo, we are using Redis Cloud. It is still faster than making an API call to the LLM. If Redis were set up locally(in the same premise as the notebook), the response would be even faster."
      ],
      "metadata": {
        "id": "NwzS3RX-7Px6"
      }
    }
  ]
}